{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridStatus Data Exploration\n",
    "\n",
    "This notebook explores energy grid data using the GridStatus library.\n",
    "\n",
    "## Setup\n",
    "First time setup:\n",
    "```bash\n",
    "pip install gridstatus pandas matplotlib plotly seaborn jupyterlab\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gridstatus\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set up paths for data caching\n",
    "DATA_DIR = '../data/raw'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Explore Available ISOs\n",
    "\n",
    "GridStatus supports multiple Independent System Operators (ISOs).\n",
    "CAISO (California) has the most solar generation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available ISOs\n",
    "print(\"Available ISOs:\")\n",
    "for iso_name in dir(gridstatus):\n",
    "    if iso_name.isupper() and not iso_name.startswith('_'):\n",
    "        print(f\"  - {iso_name}\")\n",
    "\n",
    "# Initialize CAISO (California - lots of solar)\n",
    "caiso = gridstatus.CAISO()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fetch Fuel Mix Data\n",
    "\n",
    "Fuel mix shows generation by source (solar, wind, natural gas, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define date range - start with a week of data\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=7)\n",
    "\n",
    "# Format dates for API\n",
    "start_str = start_date.strftime('%Y-%m-%d')\n",
    "end_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Fetching data from {start_str} to {end_str}...\")\n",
    "\n",
    "# Check if we have cached data\n",
    "cache_file = f'{DATA_DIR}/fuel_mix_{start_str}_{end_str}.csv'\n",
    "\n",
    "if os.path.exists(cache_file):\n",
    "    print(f\"Loading from cache: {cache_file}\")\n",
    "    fuel_mix = pd.read_csv(cache_file, parse_dates=['Time'])\n",
    "else:\n",
    "    print(\"Fetching from API...\")\n",
    "    fuel_mix = caiso.get_fuel_mix(start=start_str, end=end_str)\n",
    "    # Save to cache\n",
    "    fuel_mix.to_csv(cache_file, index=False)\n",
    "    print(f\"Saved to cache: {cache_file}\")\n",
    "\n",
    "print(f\"\\nData shape: {fuel_mix.shape}\")\n",
    "fuel_mix.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check columns and data types\n",
    "print(\"Column info:\")\n",
    "print(fuel_mix.info())\n",
    "\n",
    "print(\"\\nUnique fuel types:\")\n",
    "if 'Fuel' in fuel_mix.columns:\n",
    "    print(fuel_mix['Fuel'].unique())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "fuel_mix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quick Visualization: Solar Generation Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot solar generation over the week\n",
    "fig = px.line(\n",
    "    fuel_mix, \n",
    "    x='Time', \n",
    "    y='Solar',\n",
    "    title='Solar Generation Over Time (CAISO) - Past 7 Days',\n",
    "    labels={'Solar': 'Megawatts', 'Time': 'Date/Time'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    hovermode='x unified',\n",
    "    xaxis_title='Date/Time',\n",
    "    yaxis_title='Solar Generation (MW)',\n",
    "    height=500\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Print some interesting stats\n",
    "print(f\"\\nSolar Generation Stats:\")\n",
    "print(f\"  Peak generation: {fuel_mix['Solar'].max():,} MW\")\n",
    "print(f\"  Average daytime generation: {fuel_mix[fuel_mix['Solar'] > 0]['Solar'].mean():.0f} MW\")\n",
    "print(f\"  Peak time: {fuel_mix.loc[fuel_mix['Solar'].idxmax(), 'Time']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Two Days: Sunny vs Cloudy\n",
    "\n",
    "TODO: Pick specific dates based on patterns observed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare two specific days\n",
    "# day1 = '2024-06-15'  # Sunny day\n",
    "# day2 = '2024-06-20'  # Cloudy day\n",
    "\n",
    "# Filter data for these days and create comparison plot\n",
    "# (Implementation depends on data structure discovered above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore Other Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get load (demand) data\n",
    "# load_data = caiso.get_load(start=start_str, end=end_str)\n",
    "\n",
    "# Get pricing data (LMP - Locational Marginal Price)\n",
    "# price_data = caiso.get_lmp(start=start_str, end=end_str, market='DAY_AHEAD_HOURLY')\n",
    "\n",
    "# Explore correlations between generation, demand, and price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes & Findings\n",
    "\n",
    "Use this section to document interesting patterns:\n",
    "- [ ] Duck curve - when does solar peak vs demand peak?\n",
    "- [ ] Weekend vs weekday patterns\n",
    "- [ ] Seasonal variations\n",
    "- [ ] Weather events impact\n",
    "- [ ] Price spikes during low renewable generation\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Add Cloud Cover Data from NOAA\n",
    "\n",
    "To correlate solar generation with cloud cover, we'll use NOAA's Climate Data API.\n",
    "\n",
    "### Setup: Get NOAA API Token\n",
    "1. Visit: https://www.ncdc.noaa.gov/cdo-web/token\n",
    "2. Enter your email to receive a free API token\n",
    "3. Save it in a `.env` file (gitignored) or paste below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA API Configuration\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "# Option 1: Set your token here (or use .env file)\n",
    "NOAA_TOKEN = \"\"  # Get from: https://www.ncdc.noaa.gov/cdo-web/token\n",
    "\n",
    "# Option 2: Load from .env file (create ../data/.env with NOAA_TOKEN=your_token)\n",
    "try:\n",
    "    with open('../data/.env', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('NOAA_TOKEN='):\n",
    "                NOAA_TOKEN = line.strip().split('=')[1]\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "if not NOAA_TOKEN:\n",
    "    print(\"⚠️  Please set NOAA_TOKEN above or create ../data/.env file\")\n",
    "else:\n",
    "    print(\"✓ NOAA API token configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch hourly weather data from NOAA\n",
    "def fetch_noaa_weather(station_id: str, start_date: str, end_date: str, token: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Fetch hourly weather data including cloud cover from NOAA.\n",
    "    \n",
    "    Args:\n",
    "        station_id: NOAA station ID (e.g., 'GHCND:USW00023174' for San Francisco)\n",
    "        start_date: Start date in 'YYYY-MM-DD' format\n",
    "        end_date: End date in 'YYYY-MM-DD' format\n",
    "        token: NOAA API token\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with weather observations including cloud cover\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ncei.noaa.gov/cdo-web/api/v2/data\"\n",
    "    \n",
    "    headers = {'token': token}\n",
    "    params = {\n",
    "        'datasetid': 'NORMAL_HLY',  # Hourly data\n",
    "        'stationid': station_id,\n",
    "        'startdate': start_date,\n",
    "        'enddate': end_date,\n",
    "        'units': 'metric',\n",
    "        'limit': 1000\n",
    "    }\n",
    "    \n",
    "    cache_file = f'{DATA_DIR}/weather_{station_id.replace(\":\", \"_\")}_{start_date}_{end_date}.csv'\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading weather from cache: {cache_file}\")\n",
    "        return pd.read_csv(cache_file, parse_dates=['date'])\n",
    "    \n",
    "    print(f\"Fetching weather data from NOAA API...\")\n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        if 'results' in data:\n",
    "            df = pd.DataFrame(data['results'])\n",
    "            df.to_csv(cache_file, index=False)\n",
    "            print(f\"Saved to cache: {cache_file}\")\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"No results found. Response: {data}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Find weather stations near California solar regions\n",
    "# Major stations: San Francisco, Los Angeles, Fresno, Sacramento\n",
    "CALIFORNIA_STATIONS = {\n",
    "    'San Francisco': 'GHCND:USW00023174',\n",
    "    'Los Angeles': 'GHCND:USW00023174',\n",
    "    'Fresno': 'GHCND:USW00093193',\n",
    "    'Sacramento': 'GHCND:USW00023232'\n",
    "}\n",
    "\n",
    "print(\"California weather stations available:\")\n",
    "for city, station_id in CALIFORNIA_STATIONS.items():\n",
    "    print(f\"  {city}: {station_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch weather data for Sacramento (central CA, good solar proxy)\n",
    "if NOAA_TOKEN:\n",
    "    weather_data = fetch_noaa_weather(\n",
    "        station_id=CALIFORNIA_STATIONS['Sacramento'],\n",
    "        start_date=start_str,\n",
    "        end_date=end_str,\n",
    "        token=NOAA_TOKEN\n",
    "    )\n",
    "    \n",
    "    if weather_data is not None:\n",
    "        print(f\"\\nWeather data shape: {weather_data.shape}\")\n",
    "        print(\"\\nAvailable data types:\")\n",
    "        if 'datatype' in weather_data.columns:\n",
    "            print(weather_data['datatype'].unique())\n",
    "        weather_data.head()\n",
    "else:\n",
    "    print(\"Set NOAA_TOKEN first to fetch weather data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's explore what datasets are available\n",
    "def explore_noaa_datasets(token: str):\n",
    "    \"\"\"List available NOAA datasets\"\"\"\n",
    "    headers = {'token': token}\n",
    "    \n",
    "    # Get available datasets\n",
    "    print(\"Fetching available datasets...\")\n",
    "    response = requests.get('https://www.ncei.noaa.gov/cdo-web/api/v2/datasets', headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        datasets = response.json()\n",
    "        print(f\"\\nFound {len(datasets.get('results', []))} datasets:\\n\")\n",
    "        for ds in datasets.get('results', []):\n",
    "            print(f\"ID: {ds['id']}\")\n",
    "            print(f\"  Name: {ds['name']}\")\n",
    "            print(f\"  Coverage: {ds.get('mindate', 'N/A')} to {ds.get('maxdate', 'N/A')}\")\n",
    "            print()\n",
    "        return datasets\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None\n",
    "\n",
    "if NOAA_TOKEN:\n",
    "    datasets = explore_noaa_datasets(NOAA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Use NOAA Integrated Surface Database (ISD)\n",
    "\n",
    "The ISD has actual hourly observations including cloud cover. Data is available via:\n",
    "1. Direct download from NOAA FTP/HTTPS\n",
    "2. AWS Open Data bucket\n",
    "\n",
    "We'll use direct HTTPS access to download ISD data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISD station IDs for California \n",
    "# Found by browsing: https://www.ncei.noaa.gov/data/global-hourly/access/2024/\n",
    "ISD_STATIONS = {\n",
    "    'Sacramento': '72483023232',  # Sacramento International Airport\n",
    "}\n",
    "\n",
    "def download_isd_data(station_id: str, year: int) -> str:\n",
    "    \"\"\"\n",
    "    Download ISD data file for a given station and year.\n",
    "    \n",
    "    Args:\n",
    "        station_id: ISD station ID (e.g., '72483023232')\n",
    "        year: Year (e.g., 2024)\n",
    "    \n",
    "    Returns:\n",
    "        Path to downloaded file\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.ncei.noaa.gov/data/global-hourly/access\"\n",
    "    filename = f\"{station_id}.csv\"\n",
    "    url = f\"{base_url}/{year}/{filename}\"\n",
    "    \n",
    "    cache_file = f\"{DATA_DIR}/isd_{station_id}_{year}.csv\"\n",
    "    \n",
    "    if os.path.exists(cache_file):\n",
    "        print(f\"Loading ISD data from cache: {cache_file}\")\n",
    "        return cache_file\n",
    "    \n",
    "    print(f\"Downloading ISD data from: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Save the CSV file\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"Saved to: {cache_file}\")\n",
    "        return cache_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading ISD data: {e}\")\n",
    "        print(f\"Try browsing: https://www.ncei.noaa.gov/data/global-hourly/access/{year}/\")\n",
    "        return None\n",
    "\n",
    "# Download 2024 data for Sacramento\n",
    "print(\"Downloading 2024 weather data for Sacramento...\")\n",
    "test_file = download_isd_data(ISD_STATIONS['Sacramento'], 2024)\n",
    "\n",
    "if test_file:\n",
    "    # Load and preview the data\n",
    "    isd_data = pd.read_csv(test_file, low_memory=False)\n",
    "    print(f\"\\n✓ Successfully loaded ISD data!\")\n",
    "    print(f\"Data shape: {isd_data.shape}\")\n",
    "    print(f\"\\nColumns available ({len(isd_data.columns)} total):\")\n",
    "    for i, col in enumerate(isd_data.columns):\n",
    "        print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(isd_data.head())\n",
    "    \n",
    "    # Check for cloud cover columns\n",
    "    cloud_cols = [col for col in isd_data.columns if 'cloud' in col.lower() or 'sky' in col.lower()]\n",
    "    if cloud_cols:\n",
    "        print(f\"\\n☁️  Cloud-related columns found: {cloud_cols}\")\n",
    "else:\n",
    "    print(\"\\n❌ Failed to download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NOAA ISD station history file\n",
    "station_history_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.txt\"\n",
    "station_cache = f\"{DATA_DIR}/isd_station_history.txt\"\n",
    "\n",
    "if not os.path.exists(station_cache):\n",
    "    print(\"Downloading ISD station history...\")\n",
    "    response = requests.get(station_history_url)\n",
    "    with open(station_cache, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Saved to: {station_cache}\")\n",
    "else:\n",
    "    print(f\"Loading from cache: {station_cache}\")\n",
    "\n",
    "# Parse the fixed-width station history file\n",
    "# Column positions based on ISD documentation\n",
    "colspecs = [\n",
    "    (0, 6),      # USAF\n",
    "    (7, 12),     # WBAN\n",
    "    (13, 42),    # STATION NAME\n",
    "    (43, 45),    # CTRY (Country)\n",
    "    (48, 50),    # ST (State)\n",
    "    (51, 56),    # ICAO\n",
    "    (57, 64),    # LAT (latitude * 1000)\n",
    "    (65, 73),    # LON (longitude * 1000)\n",
    "    (74, 81),    # ELEV (elevation in meters * 10)\n",
    "    (82, 90),    # BEGIN\n",
    "    (91, 99),    # END\n",
    "]\n",
    "\n",
    "names = ['USAF', 'WBAN', 'STATION_NAME', 'CTRY', 'ST', 'ICAO', \n",
    "         'LAT', 'LON', 'ELEV', 'BEGIN', 'END']\n",
    "\n",
    "# Read fixed-width format, skip header\n",
    "stations = pd.read_fwf(station_cache, colspecs=colspecs, names=names, skiprows=20)\n",
    "\n",
    "# Filter for California stations\n",
    "ca_stations = stations[stations['ST'] == 'CA'].copy()\n",
    "\n",
    "# Convert coordinates - handle missing values marked with '+'\n",
    "ca_stations['LAT'] = pd.to_numeric(ca_stations['LAT'], errors='coerce') / 1000\n",
    "ca_stations['LON'] = pd.to_numeric(ca_stations['LON'], errors='coerce') / 1000\n",
    "ca_stations['ELEV'] = pd.to_numeric(ca_stations['ELEV'], errors='coerce') / 10\n",
    "\n",
    "# Remove stations with missing coordinates\n",
    "ca_stations = ca_stations.dropna(subset=['LAT', 'LON'])\n",
    "\n",
    "# Create station ID (USAF + WBAN)\n",
    "ca_stations['STATION_ID'] = ca_stations['USAF'].astype(str).str.zfill(6) + ca_stations['WBAN'].astype(str).str.zfill(5)\n",
    "\n",
    "# Filter for stations with recent data (active in 2024)\n",
    "ca_stations['END'] = pd.to_datetime(ca_stations['END'], format='%Y%m%d', errors='coerce')\n",
    "recent_stations = ca_stations[ca_stations['END'] >= '2024-01-01'].copy()\n",
    "\n",
    "print(f\"\\nFound {len(ca_stations)} California stations with valid coordinates\")\n",
    "print(f\"Found {len(recent_stations)} stations with data through 2024+\")\n",
    "print(f\"\\nCalifornia stations (sorted by latitude, north to south):\")\n",
    "recent_stations_sorted = recent_stations.sort_values('LAT', ascending=False)\n",
    "print(recent_stations_sorted[['STATION_ID', 'STATION_NAME', 'LAT', 'LON', 'ELEV']].head(30).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NOAA ISD station history file\n",
    "station_history_url = \"https://www.ncei.noaa.gov/pub/data/noaa/isd-history.txt\"\n",
    "station_cache = f\"{DATA_DIR}/isd_station_history.txt\"\n",
    "\n",
    "if not os.path.exists(station_cache):\n",
    "    print(\"Downloading ISD station history...\")\n",
    "    response = requests.get(station_history_url)\n",
    "    with open(station_cache, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Saved to: {station_cache}\")\n",
    "else:\n",
    "    print(f\"Loading from cache: {station_cache}\")\n",
    "\n",
    "# Parse the fixed-width station history file\n",
    "# Column positions based on ISD documentation\n",
    "colspecs = [\n",
    "    (0, 6),      # USAF\n",
    "    (7, 12),     # WBAN\n",
    "    (13, 42),    # STATION NAME\n",
    "    (43, 45),    # CTRY (Country)\n",
    "    (48, 50),    # ST (State)\n",
    "    (51, 56),    # ICAO\n",
    "    (57, 64),    # LAT (latitude * 1000)\n",
    "    (65, 73),    # LON (longitude * 1000)\n",
    "    (74, 81),    # ELEV (elevation in meters * 10)\n",
    "    (82, 90),    # BEGIN\n",
    "    (91, 99),    # END\n",
    "]\n",
    "\n",
    "names = ['USAF', 'WBAN', 'STATION_NAME', 'CTRY', 'ST', 'ICAO', \n",
    "         'LAT', 'LON', 'ELEV', 'BEGIN', 'END']\n",
    "\n",
    "# Read fixed-width format, skip header\n",
    "stations = pd.read_fwf(station_cache, colspecs=colspecs, names=names, skiprows=20)\n",
    "\n",
    "# Filter for California stations\n",
    "ca_stations = stations[stations['ST'] == 'CA'].copy()\n",
    "\n",
    "# Convert coordinates (stored as integers * 1000) to decimal degrees\n",
    "ca_stations['LAT'] = ca_stations['LAT'] / 1000\n",
    "ca_stations['LON'] = ca_stations['LON'] / 1000\n",
    "ca_stations['ELEV'] = ca_stations['ELEV'] / 10\n",
    "\n",
    "# Create station ID (USAF + WBAN)\n",
    "ca_stations['STATION_ID'] = ca_stations['USAF'].astype(str).str.zfill(6) + ca_stations['WBAN'].astype(str).str.zfill(5)\n",
    "\n",
    "# Filter for stations with recent data (active in 2024)\n",
    "ca_stations['END'] = pd.to_datetime(ca_stations['END'], format='%Y%m%d', errors='coerce')\n",
    "recent_stations = ca_stations[ca_stations['END'] >= '2024-01-01']\n",
    "\n",
    "print(f\"\\nFound {len(ca_stations)} total California stations\")\n",
    "print(f\"Found {len(recent_stations)} stations with data through 2024+\")\n",
    "print(f\"\\nTop California stations (by name):\")\n",
    "print(recent_stations[['STATION_ID', 'STATION_NAME', 'LAT', 'LON', 'ELEV']].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find California Weather Stations\n",
    "\n",
    "Let's download the NOAA ISD station inventory to find all California stations with cloud cover data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify cloud-related columns\n",
    "# Based on ISD documentation, cloud columns typically include:\n",
    "# - CIG (ceiling height)\n",
    "# - Columns starting with 'AA', 'GA', 'GD', 'GF' (sky cover/cloud layers)\n",
    "\n",
    "print(\"Searching for cloud-related columns...\")\n",
    "print(\"\\nColumns containing 'CIG' (ceiling):\")\n",
    "cig_cols = [col for col in isd_data.columns if 'CIG' in col.upper()]\n",
    "print(cig_cols if cig_cols else \"None found\")\n",
    "\n",
    "print(\"\\nColumns starting with cloud layer codes (AA, GA, GD, GF):\")\n",
    "cloud_layer_cols = [col for col in isd_data.columns if col.startswith(('AA', 'GA', 'GD', 'GF'))]\n",
    "print(cloud_layer_cols if cloud_layer_cols else \"None found\")\n",
    "\n",
    "print(\"\\nAll columns (to manually inspect):\")\n",
    "for i, col in enumerate(isd_data.columns, 1):\n",
    "    non_null = isd_data[col].notna().sum()\n",
    "    if non_null > 0:  # Only show columns with some data\n",
    "        print(f\"{i:3d}. {col:40s} - {non_null:,} non-null values ({non_null/len(isd_data)*100:.1f}%)\")\n",
    "\n",
    "# Show non-null percentage for potential cloud columns\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Looking for columns with data that might be cloud-related...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the main cloud cover columns\n",
    "print(\"Cloud Cover Column Details:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Parse the cloud data - ISD format is comma-delimited within each field\n",
    "# GF1 = Sky condition observation (total coverage)\n",
    "# GA1/2/3 = Individual cloud layers\n",
    "# CIG = Ceiling height\n",
    "\n",
    "print(\"\\n1. CIG (Ceiling) - Sample values:\")\n",
    "print(isd_data['CIG'].head(20))\n",
    "\n",
    "print(\"\\n2. GF1 (Sky Condition) - Sample values:\")\n",
    "print(isd_data['GF1'].dropna().head(20))\n",
    "\n",
    "print(\"\\n3. GA1 (Cloud Layer 1) - Sample values:\")\n",
    "print(isd_data['GA1'].dropna().head(20))\n",
    "\n",
    "print(\"\\n4. GD1 (Sky Cover Summation) - Sample values:\")\n",
    "print(isd_data['GD1'].dropna().head(20))\n",
    "\n",
    "# Parse DATE column to datetime for time series analysis\n",
    "print(\"\\n5. Date range in this data:\")\n",
    "isd_data['datetime'] = pd.to_datetime(isd_data['DATE'])\n",
    "print(f\"   Start: {isd_data['datetime'].min()}\")\n",
    "print(f\"   End: {isd_data['datetime'].max()}\")\n",
    "print(f\"   Total observations: {len(isd_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse cloud cover from GD1 (Sky Cover Summation)\n",
    "# Format: \"oktas,qualifier,quality,height,quality,quality\"\n",
    "# We want the first value (oktas: 0-8 scale, 9=missing)\n",
    "\n",
    "def parse_cloud_cover(gd1_value):\n",
    "    \"\"\"Extract cloud cover in oktas from GD1 field\"\"\"\n",
    "    if pd.isna(gd1_value):\n",
    "        return None\n",
    "    try:\n",
    "        oktas = int(str(gd1_value).split(',')[0])\n",
    "        return oktas if oktas != 9 else None  # 9 = missing data\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Parse cloud cover\n",
    "isd_data['cloud_cover_oktas'] = isd_data['GD1'].apply(parse_cloud_cover)\n",
    "\n",
    "# Convert oktas to percentage (oktas/8 * 100)\n",
    "isd_data['cloud_cover_pct'] = isd_data['cloud_cover_oktas'] / 8 * 100\n",
    "\n",
    "# Show distribution\n",
    "print(\"Cloud Cover Distribution (2024 Sacramento):\")\n",
    "print(\"=\"*70)\n",
    "print(isd_data['cloud_cover_oktas'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nCloud cover statistics:\")\n",
    "print(f\"  Mean: {isd_data['cloud_cover_oktas'].mean():.1f} oktas ({isd_data['cloud_cover_pct'].mean():.1f}%)\")\n",
    "print(f\"  Median: {isd_data['cloud_cover_oktas'].median():.1f} oktas\")\n",
    "print(f\"  Clear sky (0 oktas): {(isd_data['cloud_cover_oktas']==0).sum():,} observations\")\n",
    "print(f\"  Overcast (7-8 oktas): {(isd_data['cloud_cover_oktas']>=7).sum():,} observations\")\n",
    "\n",
    "# Preview the parsed data\n",
    "print(\"\\nSample of parsed cloud data:\")\n",
    "print(isd_data[['datetime', 'cloud_cover_oktas', 'cloud_cover_pct']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
